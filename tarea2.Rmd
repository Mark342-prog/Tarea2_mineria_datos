---
title: "tarea2"
author: |
  Grupo \#8  
  Osman Emanuel de León García - 23428  
  María José Yee Vidal - 231193  
  Andrés Rafael Chivalán Marroquín - 21534

date: "2026-02-24"
output: 
  pdf_document: default
header-includes:
  - \usepackage{graphicx}
  - \usepackage{titling}
  - \pretitle{\begin{center}\includegraphics[width=3in,height=5in]{Uvg_logo.jpg}\LARGE\\}
  - \posttitle{\end{center}}
---
\newpage


## Introducción
El aprendizaje no supervisado es una de las ramas principales de la minería de datos y del aprendizaje automático. Su objetivo es descubrir patrones, estructuras o relaciones ocultas en los datos sin utilizar etiquetas o resultados previamente conocidos.

Estas técnicas se emplean principalmente para la exploración y comprensión de grandes volúmenes de información, permitiendo tareas como la segmentación de elementos similares, la reducción de la complejidad de los datos, la detección de anomalías y el descubrimiento de asociaciones frecuentes. Su importancia radica en que facilita la generación de conocimiento a partir de datos no estructurados o sin clasificar, apoyando la toma de decisiones y la identificación de patrones que no son evidentes a simple vista.

### Objetivos
- Diferenciar los diferentes tipos de formas
- Aprender cuando aplicar los diferentes algoritmos
- Aprender a interpretar las respuestas
- Aprender a filtrar las respuestas

## Cargar librerías

```{r setup, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(Matrix)
library(ggplot2)
library(Rtsne)
library(umap)
library(edfReader)
library(fastICA)
```

# 1 SVD (Singular Value Decomposition)

## Descripción teórica

La Descomposición en Valores Singulares (SVD) es una técnica matemática que permite descomponer una matriz en tres matrices más simples que contienen información estructural de los datos.
Por medio de esta descomposición, se pueden identificar los componentes más importantes que explican la mayor parte de la información original
Permite reducir la cantidad de dimensiones sin perder demasiado contenido relevante.

Es útil en análisis de datos porque ayuda a simplificar matrices grandes y detectar patrones ocultos.

Algunas diferencias con el PCA es que el SVD es la base matemática del PCA, pero el PCA se aplica normalmente a matrices centradas, mientras que el SVD se puede aplicar directamente sobre cualquier matriz. Incluso el PCA se puede derivar matemáticamente a partir del SVD.

## Usos y aplicaciones
Principales usos:
- Reducción de dimensionalidad
- Sistemas de recomendación (por ejemplo la factorización de matrices)
- Compresión de datos
- Eliminación de ruido

Areas de aplicación:
- Sistemas de recomendación  
(Plataformas de streaming que permite identificar factores que relaciona a usuarios y productos)
- Procesamiento de imágenes  
(En la compresión de imágenes conservando ciertos valores más importantes)
- Procesamiento de lenguaje natural (LSA)  
(Permite detectar relaciones entre palabras y documentos)


## Cargar el dataset

El archivo `u.data` contiene los ratings en formato:

user_id | item_id | rating | timestamp

```{r}
ratings <- read.table("data/u.data",
                      sep = "\t",
                      header = FALSE)

colnames(ratings) <- c("user_id", "item_id", "rating", "timestamp")

head(ratings)
dim(ratings)
```

## Construcción de la matriz Usuario × Película

Para aplicar SVD es necesario convertir los datos en una matriz donde:

- Filas → usuarios  
- Columnas → películas  
- Valores → ratings  

```{r}
rating_matrix <- ratings %>%
  select(user_id, item_id, rating) %>%
  pivot_wider(names_from = item_id,
              values_from = rating,
              values_fill = 0)

user_ids <- rating_matrix$user_id

rating_matrix <- as.matrix(rating_matrix[,-1])

dim(rating_matrix)
```


## Aplicación de SVD

```{r}
svd_result <- svd(rating_matrix)

str(svd_result)
```

El resultado contiene:

- U → factores asociados a usuarios  
- D → valores singulares (importancia de componentes)  
- V → factores asociados a películas  


## Visualización de valores singulares

```{r}
plot(svd_result$d,
     type = "b",
     main = "Valores Singulares",
     xlab = "Componente",
     ylab = "Valor Singular")
```

Los primeros valores singulares concentran mayor información.


## Varianza explicada

```{r}
variance_explained <- svd_result$d^2 / sum(svd_result$d^2)

plot(cumsum(variance_explained),
     type = "b",
     main = "Varianza Acumulada",
     xlab = "Número de Componentes",
     ylab = "Proporción Acumulada")
```

```{r}
# ¿Cuántos componentes explican 80% y 90%?

components_80 <- which(cumsum(variance_explained) >= 0.80)[1]
components_90 <- which(cumsum(variance_explained) >= 0.90)[1]

components_80
components_90
```

Esta gráfica permite identificar cuántos componentes explican la mayor parte de la variabilidad del sistema.


## Reducción de dimensionalidad (SVD truncado)

Seleccionamos k componentes principales.

```{r}
k <- 20

U_k <- svd_result$u[,1:k]
D_k <- diag(svd_result$d[1:k])
V_k <- svd_result$v[,1:k]

approx_matrix <- U_k %*% D_k %*% t(V_k)

dim(approx_matrix)
```


## Error de reconstrucción

```{r}
reconstruction_error <- norm(rating_matrix - approx_matrix, type = "F")

reconstruction_error
```

Un menor error indica mejor aproximación usando k componentes.


## Comparación parcial

```{r}
original_sample <- rating_matrix[1:5,1:5]
approx_sample <- approx_matrix[1:5,1:5]

original_sample
approx_sample
```

## Interpretación de resultados

La gráfica de los valores singulares muestra que los primeros componentes tienen valores mucho mayores que el resto. Esto indica que una gran parte de la información del sistema de calificaciones está concentrada en pocas dimensiones. En otras palabras, aunque el dataset tiene muchas películas, las preferencias de los usuarios pueden resumirse en un número menor de factores importantes.

En la gráfica de varianza acumulada se observa que los primeros componentes explican un alto porcentaje de la variabilidad total. Esto significa que no es necesario usar todas las dimensiones originales para representar adecuadamente la información, ya que una parte significativa puede capturarse con menos componentes.

Al reconstruir la matriz utilizando solo 20 componentes, se obtiene un error de reconstrucción moderado. Esto indica que existe cierta pérdida de información, pero la estructura general del sistema de ratings se mantiene. Esto demuestra que el comportamiento de los usuarios puede representarse mediante factores latentes que resumen patrones de preferencia.




---

## 2. t-SNE

## Descripción teórica

t-SNE es un algoritmo de reducción de dimensionalidad diseñado principalmente para visualizar datos de alta dimensión en espacios de dos o tres dimensiones.
Su objetivo es representar los datos de forma que los puntos similares permanezcan cercanos entre sí en la nueva representación
El algoritmo transforma las distancias entre observaciones, en probabilidades, y busca preservar las relaciones locales.

Es útil para explorar estructuras complejas que no son lineales.

A diferencia del PCA, que es un método lineal basado en la maximización de la varianza, t-SNE es un algoritmo no lineal que prioriza la preservación de relaciones locales entre los datos.
El PCA busca mantener la mayor variabilidad global posible, mientras que t-SNE se enfoca en conservar la estructura de vecindad, lo que lo hace más adecuado para visualización.

## Usos y aplicaciones
Principales usos:
- Visualización de datos de alta dimensión
- Exploración de agrupamientos
- Análisis previo a clustering
- Identificación de patrones ocultos

Areas de aplicación:
- Bioinformática  
(Visualización de datos genéticos y expresión celular)
- Visión por computadora  
(Representación de características extraídas de imágenes)
- Análisis de clientes  
(Visualización de segmentos de mercado)

## Cargar dataset

```{r}
data <- read.csv("data/data.csv")

head(data)
dim(data)
```



## Limpieza de datos

Eliminamos columnas que no aportan información al modelo.

```{r}
# Ver nombres de columnas
colnames(data)

# Eliminar columna id
data <- data %>%
  select(-id)

# Eliminar columnas completamente vacías (si existen)
data <- data[, colSums(is.na(data)) < nrow(data)]
diagnosis <- data$diagnosis

data_numeric <- data %>%
  select(-diagnosis)
```

## Escalamiento de datos

```{r}
data_scaled <- scale(data_numeric)
```

## Aplicación de t-SNE

```{r}
set.seed(123)

tsne_result <- Rtsne(data_scaled,
                     dims = 2,
                     perplexity = 30,
                     verbose = TRUE,
                     max_iter = 500)
```


## Crear dataframe para visualización

```{r}
tsne_df <- data.frame(
  Dim1 = tsne_result$Y[,1],
  Dim2 = tsne_result$Y[,2],
  Diagnosis = diagnosis
)
```

## Visualización

```{r}
ggplot(tsne_df, aes(x = Dim1, y = Dim2, color = Diagnosis)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(title = "Visualización t-SNE - Breast Cancer Dataset",
       x = "Dimensión 1",
       y = "Dimensión 2") +
  theme_minimal()
```


## Interpretación de resultados

La visualización obtenida mediante t-SNE muestra una clara tendencia a la formación de dos grupos principales, correspondientes a los diagnósticos benigno (B) y maligno (M). Esto indica que las variables del dataset contienen información suficiente para diferenciar ambos tipos de tumores.

Se observa que los puntos malignos tienden a concentrarse en una región específica del espacio bidimensional, mientras que los benignos ocupan otra zona distinta. Aunque existe cierto solapamiento entre ambos grupos, la separación general es evidente.

Dado que t-SNE preserva relaciones locales entre observaciones, esta agrupación sugiere que los pacientes con características similares tienden a compartir el mismo diagnóstico. Esto demuestra la utilidad del algoritmo como herramienta de visualización para explorar la estructura interna de datos de alta dimensionalidad.


---

## 3. UMAP
## Descripción teórica

UMAP es un algoritmo de reducción de dimensionalidad que busca preservar la estructura interna de los datos al proyectarlos en un espacio de menor dimensión.

Se basa en la idea de que los datos pueden organizarse sobre estructuras de menor complejidad, y trata de mantener tanto las relaciones locales como la parte de la estructura global.

Su objetivo principal es facilitar la visualización y el análisis exploratorio, esto siempre manteniendo la geometría del conjunto de datos.

El PCA es un método lineal que reduce dimensiones manteniendo la mayor varianza posible, mientras que UMAP es un método no lineal que intenta preservar la estructura geométrica interna de los datos.
UMAP puede capturar relaciones complejas que no pueden representarse mediante combinaciones lineales, lo que le permite modelar estructuras más complejas que el PCA.

## Usos y aplicaciones
Principales usos:
- Reducción de dimensionalidad
- Visualización de datos complejos
- Preprocesamiento para clustering
- Exploración de estructuras internas

Areas de aplicación:
- Genómica  
(Análisis de datos de secuenciación genética)
- Procesamiento de texto  
(Visualización de representaciones vectoriales de palabras)
- Análisis de imágenes  
(Identificación de patrones en datos visuales)

## Aplicación de UMAP

```{r}
set.seed(123)

umap_result <- umap(data_scaled)
```



## Crear dataframe para visualización

```{r}
umap_df <- data.frame(
  Dim1 = umap_result$layout[,1],
  Dim2 = umap_result$layout[,2],
  Diagnosis = diagnosis
)
```



## Visualización

```{r}
ggplot(umap_df, aes(x = Dim1, y = Dim2, color = Diagnosis)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(title = "Visualización UMAP - Breast Cancer Dataset",
       x = "Dimensión 1",
       y = "Dimensión 2") +
  theme_minimal()
```



## Interpretación de resultados

La proyección obtenida mediante UMAP muestra una separación clara entre los casos benignos y malignos. A diferencia de t-SNE, UMAP tiende a preservar no solo las relaciones locales entre puntos, sino también parte de la estructura global del conjunto de datos.

En la visualización se observa que los dos grupos presentan menor solapamiento y una estructura más compacta, lo que sugiere que UMAP captura de forma eficiente la geometría interna de los datos.

Esto confirma que las características clínicas medidas en el dataset permiten diferenciar ambos diagnósticos y demuestra que UMAP es una herramienta efectiva para la reducción de dimensionalidad y visualización de datos complejos.



## Cargar señal ECG convertida

```{r}
ecg <- read.csv("data/100_ecg.csv")

dim(ecg)
head(ecg)
```



## Seleccionar una porción de la señal

(Usamos solo las primeras 5000 muestras para simplificar el análisis)

```{r}
signals <- as.matrix(ecg[1:5000, ])

dim(signals)
```



## Visualizar señales originales

```{r}
matplot(signals, type="l",
        main="Señales ECG Originales (Mezcladas)",
        xlab="Tiempo (muestras)",
        ylab="Amplitud")
```

---

# 4. ICA

## Descripción teórica

El Análisis de Componentes Independientes (ICA) es una técnica que busca separar un conjunto de señales observadas en componentes que sean estadísticamente independientes.

El algoritmo intenta recuperar las fuentes originales sin conocerlas previamente, y es especialmente útil cuando los datos contienen mezclas de diferentes señales y se desea identificar cada una por separado.

El PCA busca componentes que maximicen la varianza y que sean ortogonales entre sí, mientras que ICA busca componentes que sean estadísticamente independientes, es decir que, ICA no solo elimina correlaciones, sino que intenta separar fuentes subyacentes independientes, lo cual va más allá del objetivo del PCA.

## Usos y aplicaciones
Principales usos:
- Separación de señales
- Eliminación de ruido
- Análisis de señales biomédicas
- Identificación de fuentes ocultas

Areas de aplicación:
- Electroencefalografía (EEG)  
(Separación de actividad cerebral y artefactos)
- Electrocardiografía (ECG)  
(Análisis de señales cardíacas)
- Procesamiento de audio  
(Separación de múltiples voces en una grabación)

## Aplicar ICA

```{r}
set.seed(123)

ica_result <- fastICA(signals, n.comp = ncol(signals))

components <- ica_result$S

dim(components)
```

## Visualizar componentes independientes

```{r}
matplot(components, type="l",
        main="Componentes Independientes Recuperados (ICA)",
        xlab="Tiempo (muestras)",
        ylab="Amplitud")
```

## Interpretación de resultados

Las señales ECG originales corresponden a registros reales del MIT-BIH Arrhythmia Database.
Estas señales representan mediciones eléctricas cardíacas registradas mediante múltiples derivaciones,
las cuales pueden contener mezclas de diferentes fuentes fisiológicas y posibles artefactos.

Al aplicar ICA, el algoritmo busca descomponer las señales observadas en componentes estadísticamente independientes.
En la visualización de los componentes recuperados se observan patrones diferenciados, lo que indica
que ICA logró separar parcialmente las fuentes subyacentes presentes en la señal original.

Este resultado demuestra la utilidad de ICA en el análisis de señales biomédicas,
ya que permite identificar estructuras ocultas y separar posibles fuentes independientes
a partir de mezclas lineales observadas.

## Comparaciones generales 

### SVD (Singular Value Decomposition)

La SVD (Descomposición en Valores Singulares) es un método algebraico que descompone una matriz en componentes ortogonales ordenados por importancia.

#### Ventajas

- Método determinista (mismo resultado en cada ejecución).

- Base matemática sólida.

- Eficiente para datos lineales y matrices grandes.

- Muy útil en compresión de datos y análisis de texto (ej. LSA).

- Preserva la mayor varianza posible en menos dimensiones.

#### Limitaciones

- Solo captura relaciones lineales.

- No preserva estructuras locales complejas.

- Puede ser sensible a datos no escalados.

- Componentes a veces difíciles de interpretar.

### t-SNE (t-Distributed Stochastic Neighbor Embedding)

t-SNE es un algoritmo no lineal diseñado principalmente para visualización de datos de alta dimensión en 2D o 3D.

#### Ventajas

- Excelente para visualizar clusters.

- Preserva muy bien relaciones locales.

- Revela estructuras complejas no lineales.

- Muy popular en análisis de datos biológicos y NLP.

#### Limitaciones

- Alto costo computacional.

- No preserva bien distancias globales.

- No es determinista (puede variar entre ejecuciones).

- Difícil de usar para nuevos datos sin reentrenar.

- Principalmente útil para visualización, no para modelado posterior.

### UMAP (Uniform Manifold Approximation and Projection)

UMAP es un método no lineal basado en teoría de variedades y grafos, orientado tanto a visualización como a reducción eficiente de dimensionalidad.

#### Ventajas

- Más rápido que t-SNE.

- Preserva mejor la estructura global y local.

- Escalable a grandes volúmenes de datos.

- Permite transformar nuevos datos sin recalcular todo.

- Buen equilibrio entre interpretación y rendimiento.

#### Limitaciones

Requiere ajuste de hiperparámetros.

Puede generar agrupamientos artificiales si no se configura adecuadamente.

Menor estabilidad si los datos tienen mucho ruido.

### ICA (Independent Component Analysis)

Independent Component Analysis es una técnica que busca separar señales en componentes estadísticamente independientes.

#### Ventajas

- Útil para separación de señales (ej. audio, EEG).

- Identifica fuentes independientes ocultas.

- Captura estructuras no gaussianas.

- Interpretación clara cuando se conocen las fuentes subyacentes.

#### Limitaciones

- Supone independencia estadística (no siempre realista).

- Sensible al ruido.

- No ordena componentes por importancia.

- No está orientado específicamente a visualización.

### Conclusiones

#### Principales Aprendizajes

Aprendimos como utilizar los adgoritmos, como interpretarlos y como identificar sus casos de uso, con lo cual aprendimos también a preparar los datos de manera en que sean más coherentes respectivos a los casos de uso.

#### Dificultades encontradas

#### Reflexiones

Estos algoritmos son bastante útiles aunque son limitados en cuanto a sus usos, en referencia en que en un caso normal los datos deverían de ser depurados de cierta manera para que los algoritmos no tengan problemas generales o que el ruido interfiera mucho con los mismos. 