---
title: "tarea2"
author: |
  Grupo \#8  
  Osman Emanuel de León García - 23428  
  María José Yee Vidal - 231193  
  Andrés Rafael Chivalán Marroquín - 21534

date: "2026-02-24"
output: 
  pdf_document: default
header-includes:
  - \usepackage{graphicx}
  - \usepackage{titling}
  - \pretitle{\begin{center}\includegraphics[width=3in,height=5in]{Uvg_logo.jpg}\LARGE\\}
  - \posttitle{\end{center}}
---
\newpage


## Introducción
El aprendizaje no supervisado es una de las ramas principales de la minería de datos y del aprendizaje automático. Su objetivo es descubrir patrones, estructuras o relaciones ocultas en los datos sin utilizar etiquetas o resultados previamente conocidos.

Estas técnicas se emplean principalmente para la exploración y comprensión de grandes volúmenes de información, permitiendo tareas como la segmentación de elementos similares, la reducción de la complejidad de los datos, la detección de anomalías y el descubrimiento de asociaciones frecuentes. Su importancia radica en que facilita la generación de conocimiento a partir de datos no estructurados o sin clasificar, apoyando la toma de decisiones y la identificación de patrones que no son evidentes a simple vista.

### Objetivos
- Diferenciar los diferentes tipos de formas
- Aprender cuando aplicar los diferentes algoritmos
- Aprender a interpretar las respuestas
- Aprender a filtrar las respuestas

## Cargar librerías

```{r setup, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(Matrix)
library(ggplot2)
library(Rtsne)
library(umap)
library(edfReader)
library(fastICA)
```


## Cargar el dataset

El archivo `u.data` contiene los ratings en formato:

user_id | item_id | rating | timestamp

```{r}
ratings <- read.table("data/u.data",
                      sep = "\t",
                      header = FALSE)

colnames(ratings) <- c("user_id", "item_id", "rating", "timestamp")

head(ratings)
dim(ratings)
```


## Construcción de la matriz Usuario × Película

Para aplicar SVD es necesario convertir los datos en una matriz donde:

- Filas → usuarios  
- Columnas → películas  
- Valores → ratings  

```{r}
rating_matrix <- ratings %>%
  select(user_id, item_id, rating) %>%
  pivot_wider(names_from = item_id,
              values_from = rating,
              values_fill = 0)

user_ids <- rating_matrix$user_id

rating_matrix <- as.matrix(rating_matrix[,-1])

dim(rating_matrix)
```


## Aplicación de SVD

```{r}
svd_result <- svd(rating_matrix)

str(svd_result)
```

El resultado contiene:

- U → factores asociados a usuarios  
- D → valores singulares (importancia de componentes)  
- V → factores asociados a películas  


## Visualización de valores singulares

```{r}
plot(svd_result$d,
     type = "b",
     main = "Valores Singulares",
     xlab = "Componente",
     ylab = "Valor Singular")
```

Los primeros valores singulares concentran mayor información.


## Varianza explicada

```{r}
variance_explained <- svd_result$d^2 / sum(svd_result$d^2)

plot(cumsum(variance_explained),
     type = "b",
     main = "Varianza Acumulada",
     xlab = "Número de Componentes",
     ylab = "Proporción Acumulada")
```

```{r}
# ¿Cuántos componentes explican 80% y 90%?

components_80 <- which(cumsum(variance_explained) >= 0.80)[1]
components_90 <- which(cumsum(variance_explained) >= 0.90)[1]

components_80
components_90
```

Esta gráfica permite identificar cuántos componentes explican la mayor parte de la variabilidad del sistema.


## Reducción de dimensionalidad (SVD truncado)

Seleccionamos k componentes principales.

```{r}
k <- 20

U_k <- svd_result$u[,1:k]
D_k <- diag(svd_result$d[1:k])
V_k <- svd_result$v[,1:k]

approx_matrix <- U_k %*% D_k %*% t(V_k)

dim(approx_matrix)
```


## Error de reconstrucción

```{r}
reconstruction_error <- norm(rating_matrix - approx_matrix, type = "F")

reconstruction_error
```

Un menor error indica mejor aproximación usando k componentes.


## Comparación parcial

```{r}
original_sample <- rating_matrix[1:5,1:5]
approx_sample <- approx_matrix[1:5,1:5]

original_sample
approx_sample
```


## Interpretación de resultados

La gráfica de los valores singulares muestra que los primeros componentes tienen valores mucho mayores que el resto. Esto indica que una gran parte de la información del sistema de calificaciones está concentrada en pocas dimensiones. En otras palabras, aunque el dataset tiene muchas películas, las preferencias de los usuarios pueden resumirse en un número menor de factores importantes.

En la gráfica de varianza acumulada se observa que los primeros componentes explican un alto porcentaje de la variabilidad total. Esto significa que no es necesario usar todas las dimensiones originales para representar adecuadamente la información, ya que una parte significativa puede capturarse con menos componentes.

Al reconstruir la matriz utilizando solo 20 componentes, se obtiene un error de reconstrucción moderado. Esto indica que existe cierta pérdida de información, pero la estructura general del sistema de ratings se mantiene. Esto demuestra que el comportamiento de los usuarios puede representarse mediante factores latentes que resumen patrones de preferencia.



## t-SNE
## Cargar dataset

```{r}
data <- read.csv("data/data.csv")

head(data)
dim(data)
```



## Limpieza de datos

Eliminamos columnas que no aportan información al modelo.

```{r}
# Ver nombres de columnas
colnames(data)

# Eliminar columna id
data <- data %>%
  select(-id)

# Eliminar columnas completamente vacías (si existen)
data <- data[, colSums(is.na(data)) < nrow(data)]
diagnosis <- data$diagnosis

data_numeric <- data %>%
  select(-diagnosis)
```

## Escalamiento de datos

```{r}
data_scaled <- scale(data_numeric)
```

## Aplicación de t-SNE

```{r}
set.seed(123)

tsne_result <- Rtsne(data_scaled,
                     dims = 2,
                     perplexity = 30,
                     verbose = TRUE,
                     max_iter = 500)
```


## Crear dataframe para visualización

```{r}
tsne_df <- data.frame(
  Dim1 = tsne_result$Y[,1],
  Dim2 = tsne_result$Y[,2],
  Diagnosis = diagnosis
)
```

## Visualización

```{r}
ggplot(tsne_df, aes(x = Dim1, y = Dim2, color = Diagnosis)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(title = "Visualización t-SNE - Breast Cancer Dataset",
       x = "Dimensión 1",
       y = "Dimensión 2") +
  theme_minimal()
```


## Interpretación de resultados

La visualización obtenida mediante t-SNE muestra una clara tendencia a la formación de dos grupos principales, correspondientes a los diagnósticos benigno (B) y maligno (M). Esto indica que las variables del dataset contienen información suficiente para diferenciar ambos tipos de tumores.

Se observa que los puntos malignos tienden a concentrarse en una región específica del espacio bidimensional, mientras que los benignos ocupan otra zona distinta. Aunque existe cierto solapamiento entre ambos grupos, la separación general es evidente.

Dado que t-SNE preserva relaciones locales entre observaciones, esta agrupación sugiere que los pacientes con características similares tienden a compartir el mismo diagnóstico. Esto demuestra la utilidad del algoritmo como herramienta de visualización para explorar la estructura interna de datos de alta dimensionalidad.

## Aplicación de UMAP

```{r}
set.seed(123)

umap_result <- umap(data_scaled)
```

---

## Crear dataframe para visualización

```{r}
umap_df <- data.frame(
  Dim1 = umap_result$layout[,1],
  Dim2 = umap_result$layout[,2],
  Diagnosis = diagnosis
)
```

---

## Visualización

```{r}
ggplot(umap_df, aes(x = Dim1, y = Dim2, color = Diagnosis)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(title = "Visualización UMAP - Breast Cancer Dataset",
       x = "Dimensión 1",
       y = "Dimensión 2") +
  theme_minimal()
```

---

## Interpretación de resultados

La proyección obtenida mediante UMAP muestra una separación clara entre los casos benignos y malignos. A diferencia de t-SNE, UMAP tiende a preservar no solo las relaciones locales entre puntos, sino también parte de la estructura global del conjunto de datos.

En la visualización se observa que los dos grupos presentan menor solapamiento y una estructura más compacta, lo que sugiere que UMAP captura de forma eficiente la geometría interna de los datos.

Esto confirma que las características clínicas medidas en el dataset permiten diferenciar ambos diagnósticos y demuestra que UMAP es una herramienta efectiva para la reducción de dimensionalidad y visualización de datos complejos.

---

## Cargar señal ECG convertida

```{r}
ecg <- read.csv("data/100_ecg.csv")

dim(ecg)
head(ecg)
```

---

## Seleccionar una porción de la señal

(Usamos solo las primeras 5000 muestras para simplificar el análisis)

```{r}
signals <- as.matrix(ecg[1:5000, ])

dim(signals)
```

---

## Visualizar señales originales

```{r}
matplot(signals, type="l",
        main="Señales ECG Originales (Mezcladas)",
        xlab="Tiempo (muestras)",
        ylab="Amplitud")
```

---

## Aplicar ICA

```{r}
set.seed(123)

ica_result <- fastICA(signals, n.comp = ncol(signals))

components <- ica_result$S

dim(components)
```

---

## Visualizar componentes independientes

```{r}
matplot(components, type="l",
        main="Componentes Independientes Recuperados (ICA)",
        xlab="Tiempo (muestras)",
        ylab="Amplitud")
```

---

## Interpretación de resultados

Las señales ECG originales corresponden a registros reales del MIT-BIH Arrhythmia Database.
Estas señales representan mediciones eléctricas cardíacas registradas mediante múltiples derivaciones,
las cuales pueden contener mezclas de diferentes fuentes fisiológicas y posibles artefactos.

Al aplicar ICA, el algoritmo busca descomponer las señales observadas en componentes estadísticamente independientes.
En la visualización de los componentes recuperados se observan patrones diferenciados, lo que indica
que ICA logró separar parcialmente las fuentes subyacentes presentes en la señal original.

Este resultado demuestra la utilidad de ICA en el análisis de señales biomédicas,
ya que permite identificar estructuras ocultas y separar posibles fuentes independientes
a partir de mezclas lineales observadas.

## Comparaciones generales 

### SVD (Singular Value Decomposition)

La SVD (Descomposición en Valores Singulares) es un método algebraico que descompone una matriz en componentes ortogonales ordenados por importancia.

#### Ventajas

- Método determinista (mismo resultado en cada ejecución).

- Base matemática sólida.

- Eficiente para datos lineales y matrices grandes.

- Muy útil en compresión de datos y análisis de texto (ej. LSA).

- Preserva la mayor varianza posible en menos dimensiones.

#### Limitaciones

- Solo captura relaciones lineales.

- No preserva estructuras locales complejas.

- Puede ser sensible a datos no escalados.

- Componentes a veces difíciles de interpretar.

### t-SNE (t-Distributed Stochastic Neighbor Embedding)

t-SNE es un algoritmo no lineal diseñado principalmente para visualización de datos de alta dimensión en 2D o 3D.

#### Ventajas

- Excelente para visualizar clusters.

- Preserva muy bien relaciones locales.

- Revela estructuras complejas no lineales.

- Muy popular en análisis de datos biológicos y NLP.

#### Limitaciones

- Alto costo computacional.

- No preserva bien distancias globales.

- No es determinista (puede variar entre ejecuciones).

- Difícil de usar para nuevos datos sin reentrenar.

- Principalmente útil para visualización, no para modelado posterior.

### UMAP (Uniform Manifold Approximation and Projection)

UMAP es un método no lineal basado en teoría de variedades y grafos, orientado tanto a visualización como a reducción eficiente de dimensionalidad.

#### Ventajas

- Más rápido que t-SNE.

- Preserva mejor la estructura global y local.

- Escalable a grandes volúmenes de datos.

- Permite transformar nuevos datos sin recalcular todo.

- Buen equilibrio entre interpretación y rendimiento.

#### Limitaciones

Requiere ajuste de hiperparámetros.

Puede generar agrupamientos artificiales si no se configura adecuadamente.

Menor estabilidad si los datos tienen mucho ruido.

### ICA (Independent Component Analysis)

Independent Component Analysis es una técnica que busca separar señales en componentes estadísticamente independientes.

#### Ventajas

- Útil para separación de señales (ej. audio, EEG).

- Identifica fuentes independientes ocultas.

- Captura estructuras no gaussianas.

- Interpretación clara cuando se conocen las fuentes subyacentes.

#### Limitaciones

- Supone independencia estadística (no siempre realista).

- Sensible al ruido.

- No ordena componentes por importancia.

- No está orientado específicamente a visualización.

### Conclusiones

#### Principales Aprendizajes

Aprendimos como utilizar los adgoritmos, como interpretarlos y como identificar sus casos de uso, con lo cual aprendimos también a preparar los datos de manera en que sean más coherentes respectivos a los casos de uso.

#### Dificultades encontradas

#### Reflexiones

Estos algoritmos son bastante útiles aunque son limitados en cuanto a sus usos, en referencia en que en un caso normal los datos deverían de ser depurados de cierta manera para que los algoritmos no tengan problemas generales o que el ruido interfiera mucho con los mismos. 